{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCrywzq8MMmi5X19AZRD23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NursimaErkoc/CNN_ile_cicek_siniflandirma/blob/main/CNN_cicek_t%C3%BCr%C3%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OU4svU7lCU0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "flowers dataset :\n",
        "rgb: 224x224\n",
        "CNN ile siniflandirma modeli oluşturma ve problemi cözme\n",
        "\"\"\"\n",
        "\n",
        "#import libraries\n",
        "!pip install tensorflow matplotlib tensorflow_datasets\n",
        "\n",
        "from tensorflow_datasets import load # veri seti yükleme\n",
        "from tensorflow.data import AUTOTUNE # veri seti optimizasyonu\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import(\n",
        "    Conv2D, # 2D convolutional layer\n",
        "    MaxPooling2D, # max pooling layer\n",
        "    Flatten, # çok boyutlu veriyi tek boyutlu hale getirme düzleştirme\n",
        "    Dense, # fully connected layer tam bağlantılı katman\n",
        "    Dropout  #rastgele noronları kapatma ve overfittingi engelleme\n",
        "    )\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, #erken durdurma\n",
        "    ReduceLROnPlateau, # ogrenme oranını azaltma\n",
        "    ModelCheckpoint # model kaydetme\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_val), ds_info=load(\n",
        "    \"tf_flowers\" ,# veri seti ismi\n",
        "    split = [\"train[:80%]\", # veri setinin %80i eğitim için\n",
        "             \"train[80%:]\"], # veri setinin kalan % 20 si test için (80yüzdesinden sonraki kısım )\n",
        "    as_supervised=True, # veri setinin gorsel etiket cıftının olması\n",
        "    with_info=True # veri seti hakkında bilgi alma\n",
        ")\n",
        "print (ds_info.features) # veri seti hakkımda bilgi yazdirma\n",
        "print (\"Number of classes\", ds_info.features[\"label\"].num_classes)\n"
      ],
      "metadata": {
        "id": "w4CmVkw7qZB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ornek veri görselleştirme\n",
        "# eğitim setinde rastgele 3 resim ve etiket alma\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "for i, (image,label ) in enumerate(ds_train.take(3)):\n",
        "  ax = fig.add_subplot(1,3,i+1)\n",
        "  ax.imshow(image.numpy().astype(\"uint8\")) #resim görselleştirme\n",
        "  ax.set_title(f\"Etiket : {label.numpy()}\") # etiketi başlık olarak yzadırma\n",
        "  ax.axis(\"off\") # eksenleri kapatma\n",
        "\n",
        "plt.tight_layout()# orantılı görselleştirme\n",
        "plt.show()# grafiği gösterme\n",
        "\n"
      ],
      "metadata": {
        "id": "ccLjuhL6sch_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "IMG_SIZE = (180, 180)\n",
        "\n",
        "# data augmentation + preprocessing\n",
        "def preprocess_train(image, label):\n",
        "    \"\"\"\n",
        "    resize\n",
        "    random flip\n",
        "    brightness\n",
        "    contrast\n",
        "    crop\n",
        "    normalize\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(image, IMG_SIZE) # boyutlandırma\n",
        "    image = tf.image.random_flip_left_right(image) # yatay olarak cevirme\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1) # rastgele parlaklık\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.2) # rastgele kontrast\n",
        "    image = tf.image.random_crop(image, size=(160, 160, 3)) # rastgele kırpma\n",
        "    image = tf.image.resize(image, IMG_SIZE) #tekrar boyutlandırma\n",
        "\n",
        "    image = tf.cast(image, tf.float32) / 255.0 # normalize etme\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def preprocess_val(image, label):\n",
        "    \"\"\"\n",
        "    resize + normalize\n",
        "    (validation/test setine augmentation uygulanmaz)\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# train dataset\n",
        "ds_train = (\n",
        "    ds_train\n",
        "    .map(preprocess_train, num_parallel_calls=AUTOTUNE) # ön işleme ve agumentasyon\n",
        "    .shuffle(1000) # kariştirma\n",
        "    .batch(32)# batch boyutu\n",
        "    .prefetch(AUTOTUNE) # veri setini önceden hazırlamak\n",
        ")\n",
        "\n",
        "# validation dataset\n",
        "ds_val = (\n",
        "    ds_val\n",
        "    .map(preprocess_val, num_parallel_calls=AUTOTUNE) # ön işleme\n",
        "    .batch(32)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "9YJUW-1VuGgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN modelini oluşturma\n",
        "model= Sequential ([\n",
        "\n",
        "    # Feature Extraction Layers\n",
        "    Conv2D(32,(3,3), activation= \"relu\" , input_shape= (*IMG_SIZE,3)), #32 filtre, 3x3 karnel,relu aktivasyon 3 kanalRGB\n",
        "    MaxPooling2D((2,2)), # 2x2 max pooling\n",
        "\n",
        "    Conv2D(64,(3,3), activation= \"relu\"), # 64 filtre 3x3 karnel relu aktivasyon\n",
        "    MaxPooling2D((2,2)) ,\n",
        "\n",
        "    Conv2D(128,(3,3), activation= \"relu\"),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    # Classification Layers\n",
        "    Flatten(), # çok boyutlu veriyi tek boyutlu hale getirme\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.5), # overfittingi enngellemek için\n",
        "    Dense(ds_info.features[\"label\"].num_classes, activation = \"softmax\" )  # çıkıs katmanı softmax aktivasyonu\n",
        "])\n",
        "\n",
        "    # 3x3 kernel kaydırma penceresi küçük olması komşu pikseller arasındaki ince bağlantıyı\n",
        "    # yeterli ve hasaplama açısından verimli ardışık 3x3 ler tek bir büyüğe 5x5 denk gelebilecek daha geniş bir perspektif sunar\n",
        "    # art arda 3x3 kullanmak hem paremetre sayısını hemde hesaplama yükünü azaltıyor detaylara erişmek isityorsak küçük kullanacağız\n",
        "\n",
        "    # Ağ derinleştikçe tespit edilebilecek özellikler karmaşıklaşıyor\n",
        "    # hiyerarşik öğrenme söz konusu ilk başta kenarları ayırtedebilirken derinlesştikçe\n",
        "    # yaprak kıvrımını tespit edebiliyor bu nedenle filtre sayısı arttırlıyor\n",
        "\n",
        "    # Maxpoolingte 2x2 geniş alana bakarken hesap yükünü azaltmak ve fazladan ayrıntılardan kurtulmak için yapılıyor\n",
        "    # uzaysal boyutları küçültüp katmanların derinlik boyutunu arttırıyoruz 32 64 128\n",
        "    # modelin karmaşıklığını arttırıken maxpooling ile karmaşık azaltmak çelişiyor gibi görünsede\n",
        "    # paremetre sayısını makul tutarken temsil gücü korunur\n",
        "    # Maxpooling kullanılmazsa model çok yavaş çalışır\n",
        "    # Hesap kapasite ve ifade gücü dengesi kurulmaya çalışılıyor\n",
        "\n",
        "    # 32,64,128 sırasıyla giderken ilk katman çizgileri öğenirken son katman ince ayrıntıları öğrenmeye çalışıyor\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3TcgiZk37bvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callbacks\n",
        "callbacks= [\n",
        "    # eger val loss 3 epoch boyunca iyileşmezse eğitimi durdur ve en iyi ağırlıkları yükle\n",
        "    EarlyStopping(monitor= \"val_loss\", patience=3 , restore_best_weights = True ) ,\n",
        "\n",
        "    # val loss 2 epoch boyuncaiyileşmezse learning rate 0.2 carpani ile azaltılıyor lr azaltma daha küçük adımlarla daha çok öğrenme gerçekleşir\n",
        "    ReduceLROnPlateau(monitor=\"vall_loss\", factor =0.2 , patience = 2, verbose =1 , min_lr= 1e-9), # öğrenme oranını azalatma\n",
        "\n",
        "    #her epoch sonunda eğer model daha iyi ise kaydolur\n",
        "    ModelCheckpoint(\"best_model.h5\", save_best_only= True ) # model kaydetme en iyi modeli kaydet\n",
        "]\n"
      ],
      "metadata": {
        "id": "Kj0O7KgMRreU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#derleme\n",
        "model.compile(\n",
        "    optimizer= Adam(learning_rate=0.001) ,\n",
        "    loss= \"sparse_categorical_crossentropy\", # kayıp fonksiyonu , etiketler tamsayi olduugu icin sparse kullan\n",
        "    metrics = [\"accuracy\"]\n",
        ")\n",
        "print (model.summary()) # model özeti\n",
        "\n"
      ],
      "metadata": {
        "id": "D7PBkTcuTjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#traning\n",
        "history= model.fit(\n",
        "    ds_train, # eğitim  veri seti\n",
        "    validation_data= ds_val,\n",
        "    epochs=10,\n",
        "    callbacks= callbacks,\n",
        "    verbose=1 # eğitim ilerlemesini göster\n",
        ")\n"
      ],
      "metadata": {
        "id": "HDdKH01xUs65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# doğruluk grafiği\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Egitim Dogrulugu\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label= \"Validasyon Dogrulugu\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title (\"Model Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "#loss plot\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history[\"loss\"], label= \"Egitim Kaybi\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validasyon Kaybi\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Model Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hGR0j3bVVj2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}